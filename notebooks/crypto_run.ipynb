{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to simulate\n",
    "1. perform model retrain every n days\n",
    "2. model retrain using all historical data (normalize price with first day open price for each game)\n",
    "3. rolling forward account balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read whole simulation data\n",
    "stock_history_df = pd.concat([pd.read_csv(i) for i in ['./ETH_hist_test.csv', './ETH_hist.csv']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_history_df.sort_values(by = 'time', inplace = True)\n",
    "stock_history_df.fillna(1e-10, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjcp</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_6</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>dx_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>132.612274</td>\n",
       "      <td>133.732681</td>\n",
       "      <td>128.798157</td>\n",
       "      <td>129.610855</td>\n",
       "      <td>129.610855</td>\n",
       "      <td>8.936866e+09</td>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>129.630661</td>\n",
       "      <td>132.835358</td>\n",
       "      <td>129.198288</td>\n",
       "      <td>130.802002</td>\n",
       "      <td>130.802002</td>\n",
       "      <td>7.935230e+09</td>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>6.666667e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        open        high         low       close       adjcp  \\\n",
       "0  2019-12-31  132.612274  133.732681  128.798157  129.610855  129.610855   \n",
       "1  2020-01-01  129.630661  132.835358  129.198288  130.802002  130.802002   \n",
       "\n",
       "         volume      tic        cci_30        rsi_30        rsi_14  \\\n",
       "0  8.936866e+09  ETH-USD  1.000000e-10  1.000000e-10  1.000000e-10   \n",
       "1  7.935230e+09  ETH-USD  6.666667e+01  1.000000e+02  1.000000e+02   \n",
       "\n",
       "          rsi_6         dx_30         dx_14  \n",
       "0  1.000000e-10  1.000000e-10  1.000000e-10  \n",
       "1  1.000000e+02  1.000000e-10  1.000000e-10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_history_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use first 250 for first model\n",
    "start_trade_index = 250\n",
    "# consequence model retrain point\n",
    "model_retrain_interval = 30\n",
    "\n",
    "tech_indicators = ['cci_30',\n",
    " 'rsi_30',\n",
    " 'rsi_14',\n",
    " 'rsi_6',\n",
    " 'dx_30', \n",
    " 'dx_14']\n",
    "\n",
    "cwd = './CryptoModel/model_%i.pkl'\n",
    "reward_on_value = True\n",
    "lookback_n = 7\n",
    "\n",
    "config_max_step = model_retrain_interval\n",
    "\n",
    "if reward_on_value:\n",
    "    reward_scaling = 2 ** -10\n",
    "else:\n",
    "    reward_scaling = 2 ** -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_env.single_crypto_env import CryptoTradingEnv\n",
    "\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan, VecNormalize\n",
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tmp/sb3_log/\n"
     ]
    }
   ],
   "source": [
    "tmp_path = \"./tmp/sb3_log/\"\n",
    "# set up logger\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTraining(time_idx, px_df):\n",
    "    # create env\n",
    "    config = dict()\n",
    "\n",
    "    config['price_array'] = px_df.iloc[:time_idx][['open', 'adjcp', 'low', 'high']].values\n",
    "    config['tech_array'] = px_df.iloc[:time_idx][tech_indicators].values\n",
    "    # randomly inital states for training\n",
    "    config['if_sequence'] = False\n",
    "    config['if_randomV'] = True\n",
    "    config['if_value'] = reward_on_value\n",
    "    config['lookback_n'] = lookback_n\n",
    "\n",
    "    initial_capital = 1e-5\n",
    "    initial_stocks = np.array([40.0])\n",
    "    max_step = config_max_step\n",
    "    \n",
    "    crypto_env = CryptoTradingEnv(config, \n",
    "                              initial_capital=initial_capital,\n",
    "                              initial_stocks=initial_stocks,\n",
    "                              max_step = max_step, \n",
    "                              reward_scaling = reward_scaling\n",
    "                              )\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda : crypto_env])\n",
    "    env_train = VecCheckNan(env_train, raise_exception=True)\n",
    "    #env_train = VecNormalize(env_train)\n",
    "\n",
    "    model = DDPG(\"MlpPolicy\", env_train, learning_rate=0.00025, \n",
    "                     batch_size=128, gamma = 0.99, seed=312)\n",
    "\n",
    "    model.set_logger(new_logger)\n",
    "    \n",
    "    model.learn(total_timesteps=5e3, tb_log_name = 'ddpg', log_interval=1000)\n",
    "    print('Training finished!')\n",
    "    \n",
    "    model.save(cwd%(time_idx))\n",
    "    print('Trained model saved in ' + str(cwd%(time_idx)))\n",
    "    return cwd%(time_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelRun(start_idx, px_df, input_amount, input_stocks, last_model):\n",
    "    test_config = dict()\n",
    "\n",
    "    test_config['price_array'] = px_df.iloc[:(start_idx + config_max_step)][['open', 'adjcp', 'low', 'high']].values\n",
    "    test_config['tech_array'] = px_df.iloc[:(start_idx + config_max_step)][tech_indicators].values\n",
    "\n",
    "    #randomly start day index for back testing\n",
    "    test_config['if_sequence'] = True\n",
    "    # disable random initial capital \n",
    "    test_config['if_randomV'] = False\n",
    "\n",
    "    test_config['if_value'] = reward_on_value\n",
    "    test_config['lookback_n'] = lookback_n\n",
    "\n",
    "    max_step = min(config_max_step, px_df.shape[0] - start_idx) - 1\n",
    "    \n",
    "    print ('Run model from ', start_idx, ' to ', start_idx + max_step)\n",
    "    \n",
    "    test_env = CryptoTradingEnv(test_config, \\\n",
    "                            initial_capital=input_amount, \\\n",
    "                            max_step = max_step, \\\n",
    "                           initial_stocks = input_stocks, \n",
    "                           reward_scaling = reward_scaling, \\\n",
    "                            start_idx = start_idx)\n",
    "    state = test_env.reset()\n",
    "\n",
    "    #test_model = PPO.load(cwd)\n",
    "    test_model = DDPG.load(last_model)\n",
    "    test_model = test_model.policy.eval()\n",
    "    \n",
    "    done = False  \n",
    "    while not done:\n",
    "        action = test_model.predict(state)[0]\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "        \n",
    "    return test_env.amount, test_env.stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model at time  250\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_250.pkl\n",
      "Applying model\n",
      "Run model from  250  to  279\n",
      "initial stock: [40.] inital amount:  0.01\n",
      "initial asset:  14106.949453125\n",
      "[Day 251] BUY: 0.0\n",
      "[Day 252] SELL: 15.5364\n",
      "[Day 254] SELL: 24.460700000000003\n",
      "[Day 255] SELL: 0.0028\n",
      "[Day 256] BUY: 37.1059\n",
      "[Day 257] BUY: 0.015600000000000001\n",
      "[Day 258] BUY: 0.0001\n",
      "[Day 259] BUY: 0.0\n",
      "[Day 260] BUY: 0.0\n",
      "[Day 261] BUY: 0.0\n",
      "[Day 262] BUY: 0.0\n",
      "[Day 263] BUY: 0.0\n",
      "[Day 266] BUY: 0.0\n",
      "[Day 267] BUY: 0.0\n",
      "[Day 269] BUY: 0.0\n",
      "[Day 270] BUY: 0.0\n",
      "[Day 271] BUY: 0.0\n",
      "[Day 272] BUY: 0.0\n",
      "[Day 273] BUY: 0.0\n",
      "[Day 274] BUY: 0.0\n",
      "[Day 276] BUY: 0.0\n",
      "[Day 277] BUY: 0.0\n",
      "[Day 279] BUY: 0.0\n",
      "Episode Return:  0.8973805758585037\n",
      "Training model at time  280\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_280.pkl\n",
      "Applying model\n",
      "Run model from  280  to  309\n",
      "initial stock: [37.121696] inital amount:  0.019220727116744753\n",
      "initial asset:  12700.085626977117\n",
      "[Day 281] BUY: 0.0\n",
      "[Day 285] BUY: 0.0\n",
      "[Day 286] BUY: 0.0\n",
      "[Day 290] BUY: 0.0\n",
      "[Day 291] BUY: 0.0\n",
      "[Day 294] BUY: 0.0\n",
      "[Day 295] BUY: 0.0\n",
      "[Day 296] BUY: 0.0\n",
      "[Day 297] BUY: 0.0\n",
      "[Day 298] BUY: 0.0\n",
      "[Day 300] BUY: 0.0\n",
      "[Day 301] BUY: 0.0\n",
      "[Day 303] BUY: 0.0\n",
      "[Day 305] BUY: 0.0\n",
      "[Day 308] BUY: 0.0\n",
      "[Day 309] BUY: 0.0\n",
      "Episode Return:  1.3257187684792493\n",
      "Training model at time  310\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_310.pkl\n",
      "Applying model\n",
      "Run model from  310  to  339\n",
      "initial stock: [37.121696] inital amount:  0.019220727116744753\n",
      "initial asset:  16488.105158227117\n",
      "[Day 311] SELL: 37.1216\n",
      "[Day 313] SELL: 0.0\n",
      "[Day 314] BUY: 35.9915\n",
      "[Day 315] BUY: 0.0005\n",
      "[Day 317] BUY: 0.0039000000000000003\n",
      "[Day 319] BUY: 0.0\n",
      "[Day 320] BUY: 0.0\n",
      "[Day 321] BUY: 0.0\n",
      "[Day 323] BUY: 0.0\n",
      "[Day 324] BUY: 0.0\n",
      "[Day 329] BUY: 0.0\n",
      "[Day 332] BUY: 0.0\n",
      "[Day 333] BUY: 0.0\n",
      "[Day 334] BUY: 0.0\n",
      "[Day 335] BUY: 0.0\n",
      "[Day 336] BUY: 0.0\n",
      "[Day 338] BUY: 0.0\n",
      "Episode Return:  1.2112719477506664\n",
      "Training model at time  340\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_340.pkl\n",
      "Applying model\n",
      "Run model from  340  to  369\n",
      "initial stock: [35.995995] inital amount:  0.0030778485703044645\n",
      "initial asset:  20642.95424972357\n",
      "[Day 341] SELL: 35.9957\n",
      "[Day 343] SELL: 0.0001\n",
      "[Day 345] BUY: 34.603100000000005\n",
      "[Day 346] SELL: 30.0636\n",
      "[Day 347] BUY: 30.3974\n",
      "[Day 349] BUY: 0.0012000000000000001\n",
      "[Day 350] BUY: 0.0\n",
      "[Day 351] BUY: 0.0\n",
      "[Day 353] BUY: 0.0\n",
      "[Day 354] BUY: 0.0\n",
      "[Day 355] BUY: 0.0\n",
      "[Day 357] BUY: 0.0\n",
      "[Day 358] BUY: 0.0\n",
      "[Day 360] BUY: 0.0\n",
      "[Day 361] BUY: 0.0\n",
      "[Day 362] BUY: 0.0\n",
      "[Day 363] SELL: 20.6767\n",
      "[Day 364] BUY: 10.0358\n",
      "[Day 367] BUY: 7.932200000000001\n",
      "[Day 368] BUY: 0.0\n",
      "Episode Return:  1.9136407973744012\n",
      "Training model at time  370\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_370.pkl\n",
      "Applying model\n",
      "Run model from  370  to  399\n",
      "initial stock: [32.2296] inital amount:  0.08614935429795878\n",
      "initial asset:  39455.4689618543\n",
      "[Day 371] SELL: 32.2295\n",
      "[Day 372] SELL: 0.0\n",
      "[Day 375] SELL: 0.0\n",
      "[Day 377] BUY: 33.554500000000004\n",
      "[Day 378] BUY: 0.0004\n",
      "[Day 379] BUY: 0.0\n",
      "[Day 380] BUY: 0.0\n",
      "[Day 382] BUY: 0.0\n",
      "[Day 383] BUY: 0.0\n",
      "[Day 384] BUY: 0.0\n",
      "[Day 386] BUY: 0.0\n",
      "[Day 387] BUY: 0.0\n",
      "[Day 388] BUY: 0.0\n",
      "[Day 389] BUY: 0.0\n",
      "[Day 390] BUY: 0.0\n",
      "[Day 392] BUY: 0.0\n",
      "[Day 394] BUY: 0.0\n",
      "[Day 397] BUY: 0.0\n",
      "[Day 398] BUY: 0.0\n",
      "[Day 399] BUY: 0.0\n",
      "Episode Return:  1.4269302998541342\n",
      "Training model at time  400\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_400.pkl\n",
      "Applying model\n",
      "Run model from  400  to  429\n",
      "initial stock: [33.555] inital amount:  0.05571912424407466\n",
      "initial asset:  54165.46978162424\n",
      "[Day 401] SELL: 33.5272\n",
      "[Day 403] BUY: 31.3362\n",
      "[Day 404] BUY: 0.0\n",
      "[Day 405] BUY: 0.0\n",
      "[Day 406] BUY: 0.0\n",
      "[Day 407] BUY: 0.0\n",
      "[Day 408] BUY: 0.0\n",
      "[Day 409] BUY: 0.0\n",
      "[Day 410] BUY: 0.0\n",
      "[Day 413] BUY: 0.0\n",
      "[Day 414] BUY: 0.0\n",
      "[Day 415] BUY: 0.0\n",
      "[Day 417] BUY: 0.0\n",
      "[Day 418] BUY: 0.0\n",
      "[Day 419] BUY: 0.0\n",
      "[Day 420] BUY: 0.0\n",
      "[Day 421] BUY: 0.0\n",
      "[Day 422] BUY: 0.0\n",
      "[Day 423] BUY: 0.0\n",
      "[Day 424] BUY: 0.0\n",
      "[Day 425] BUY: 0.0\n",
      "[Day 427] BUY: 0.0\n",
      "[Day 429] BUY: 0.0\n",
      "Episode Return:  1.06238205193994\n",
      "Training model at time  430\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_430.pkl\n",
      "Applying model\n",
      "Run model from  430  to  459\n",
      "initial stock: [31.364] inital amount:  0.016680892775184475\n",
      "initial asset:  58589.501055892775\n",
      "[Day 431] SELL: 31.361\n",
      "[Day 432] BUY: 31.6016\n",
      "[Day 433] BUY: 0.0\n",
      "[Day 434] BUY: 0.0\n",
      "[Day 435] BUY: 0.0\n",
      "[Day 438] BUY: 0.0\n",
      "[Day 439] BUY: 0.0\n",
      "[Day 440] BUY: 0.0\n",
      "[Day 442] BUY: 0.0\n",
      "[Day 443] BUY: 0.0\n",
      "[Day 445] BUY: 0.0\n",
      "[Day 449] BUY: 0.0\n",
      "[Day 450] BUY: 0.0\n",
      "[Day 452] BUY: 0.0\n",
      "[Day 455] BUY: 0.0\n",
      "[Day 456] BUY: 0.0\n",
      "[Day 457] BUY: 0.0\n",
      "[Day 458] BUY: 0.0\n",
      "[Day 459] BUY: 0.0\n",
      "Episode Return:  1.0632484436855099\n",
      "Training model at time  460\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_460.pkl\n",
      "Applying model\n",
      "Run model from  460  to  489\n",
      "initial stock: [31.6046] inital amount:  0.08643898852460552\n",
      "initial asset:  66008.62550148852\n",
      "[Day 462] BUY: 0.0\n",
      "[Day 464] BUY: 0.0\n",
      "[Day 465] BUY: 0.0\n",
      "[Day 468] SELL: 3.8400000000000003\n",
      "[Day 469] BUY: 2.0237000000000003\n",
      "[Day 472] BUY: 1.3253000000000001\n",
      "[Day 473] BUY: 0.6888000000000001\n",
      "[Day 474] BUY: 0.0082\n",
      "[Day 475] SELL: 19.7394\n",
      "[Day 477] BUY: 20.486700000000003\n",
      "[Day 480] SELL: 30.9055\n",
      "[Day 481] BUY: 29.623\n",
      "[Day 482] SELL: 30.1114\n",
      "[Day 483] BUY: 29.9131\n",
      "[Day 486] SELL: 27.081100000000003\n",
      "[Day 487] BUY: 25.804000000000002\n",
      "[Day 489] SELL: 21.1412\n",
      "Episode Return:  1.5795042018973044\n",
      "Training model at time  490\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_490.pkl\n",
      "Applying model\n",
      "Run model from  490  to  519\n",
      "initial stock: [8.658805] inital amount:  74087.3134504417\n",
      "initial asset:  107879.5790754417\n",
      "[Day 492] SELL: 8.658800000000001\n",
      "[Day 493] SELL: 0.0\n",
      "[Day 494] SELL: 0.0\n",
      "[Day 495] SELL: 0.0\n",
      "[Day 496] SELL: 0.0\n",
      "[Day 497] BUY: 27.637600000000003\n",
      "[Day 498] BUY: 0.0286\n",
      "[Day 500] BUY: 0.001\n",
      "[Day 501] BUY: 0.0\n",
      "[Day 502] BUY: 0.0\n",
      "[Day 503] BUY: 0.0\n",
      "[Day 505] BUY: 0.0001\n",
      "[Day 506] BUY: 0.0\n",
      "[Day 508] BUY: 0.0\n",
      "[Day 509] BUY: 0.0\n",
      "[Day 512] BUY: 0.0\n",
      "[Day 513] BUY: 0.0\n",
      "[Day 514] BUY: 0.0\n",
      "[Day 515] BUY: 0.0\n",
      "[Day 516] BUY: 0.0\n",
      "[Day 517] BUY: 0.0\n",
      "[Day 518] BUY: 0.0\n",
      "[Day 519] BUY: 0.0\n",
      "Episode Return:  0.6963256779361766\n",
      "Training model at time  520\n",
      "Training finished!\n",
      "Trained model saved in ./CryptoModel/model_520.pkl\n",
      "Applying model\n",
      "Run model from  520  to  549\n",
      "initial stock: [27.667305] inital amount:  0.024160176310582898\n",
      "initial asset:  71665.62572267631\n",
      "[Day 522] BUY: 0.0\n",
      "[Day 523] SELL: 6.1345\n",
      "[Day 525] BUY: 6.4924\n",
      "[Day 526] BUY: 0.0\n",
      "[Day 528] BUY: 0.0\n",
      "[Day 530] BUY: 0.0\n",
      "[Day 531] BUY: 0.0\n",
      "[Day 533] BUY: 0.0\n",
      "[Day 534] BUY: 0.0\n",
      "[Day 536] BUY: 0.0\n",
      "[Day 537] BUY: 0.0\n",
      "[Day 538] BUY: 0.0\n",
      "[Day 540] BUY: 0.0\n",
      "[Day 543] SELL: 0.6559\n",
      "[Day 544] SELL: 0.7708\n",
      "[Day 546] BUY: 1.0496\n",
      "[Day 548] BUY: 0.14400000000000002\n",
      "[Day 549] SELL: 3.3077\n",
      "Episode Return:  0.9058884832833051\n",
      "Training model at time  550\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fabf69bec2a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_trade_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_history_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_retrain_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Training model at time '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_history_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Applying model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-63a180b89307>\u001b[0m in \u001b[0;36mmodelTraining\u001b[0;34m(time_idx, px_df)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ddpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mlearning_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             )\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;31m# Select action randomly or according to policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0;31m# Rescale and perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36m_sample_action\u001b[0;34m(self, learning_starts, action_noise)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# we assume that the policy uses tanh to scale the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0munscaled_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# Rescale the action from [low, high] to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \"\"\"\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Convert to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/td3/policies.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Note: the deterministic deterministic parameter is ignored in the case of TD3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m#   Predictions are always deterministic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/python/DRL/NeoFinRL/stable_baselines3/td3/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# assert deterministic, 'The TD3 actor only outputs deterministic actions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_amount = 0.01\n",
    "test_stocks = np.array([40.0])\n",
    "\n",
    "for t in range(start_trade_index, stock_history_df.shape[0], model_retrain_interval):\n",
    "    print ('Training model at time ', t)\n",
    "    model_file = modelTraining(t, stock_history_df)\n",
    "    \n",
    "    print ('Applying model')\n",
    "    test_amount, test_stocks = modelRun(t, stock_history_df, test_amount, test_stocks, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
